{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step for text generation in RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in Text Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text Processing and Vectorization \n",
    "\n",
    "# ----> we will encode them each to an integer\n",
    "# ----> eg:\n",
    "#           A : 1, \n",
    "#           B : 2,\n",
    "#           C : 3, \n",
    "#           ? : 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Creating Batches\n",
    "\n",
    "# -----> we'll use tensorflow dataset object to easily create batches of text sequences.\n",
    "\n",
    "# -----> eg :\n",
    "#              ---  [\"h\",\"e\",\"l\",\"l\",\"o\",\"\",\"m\"]\n",
    "\n",
    "#              ---  [\"e\",\"l\",\"l\",\"o\",\"\",\"m\",\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. creating the model\n",
    "\n",
    "# ----> 3 layers in model\n",
    "# ----------> Embedding\n",
    "# ----------> GRU\n",
    "# ----------> Dense\n",
    "\n",
    "\n",
    "\n",
    "# 1. Embedding layers (explanation) :\n",
    "\n",
    "\"\"\"\n",
    "The most common application of this layer is for text processing. Let's see a simple example. Our training set consists only of two phrases:\n",
    "\n",
    "---> Hope to see you soon\n",
    "\n",
    "---> Nice to see you again\n",
    "\n",
    "So we can encode these phrases by assigning each word a unique integer number (by order of appearance in our training dataset for example). Then our phrases could be rewritten as:\n",
    "\n",
    "---> [0, 1, 2, 3, 4]\n",
    "\n",
    "---> [5, 1, 2, 3, 6]\n",
    "\n",
    "Now imagine we want to train a network whose first layer is an embeding layer. In this case, we should initialize it as follows:\n",
    "\n",
    "------------>  Embedding(7, 2, input_length=5)\n",
    "\n",
    "The first argument (7) is the number of distinct words in the training set. The second argument (2) indicates the size of the embedding vectors. The input_length argumet, of course, determines the size of each input sequence.\n",
    "\n",
    "Once the network has been trained, we can get the weights of the embedding layer, which in this case will be of size (7, 2) and can be thought as the table used to map integers to embedding vectors:\n",
    "\n",
    "+------------+------------+\n",
    "|   index    |  Embedding |\n",
    "+------------+------------+\n",
    "|     0      | [1.2, 3.1] |\n",
    "|     1      | [0.1, 4.2] |\n",
    "|     2      | [1.0, 3.1] |\n",
    "|     3      | [0.3, 2.1] |\n",
    "|     4      | [2.2, 1.4] |\n",
    "|     5      | [0.7, 1.7] |\n",
    "|     6      | [4.1, 2.0] |\n",
    "+------------+------------+\n",
    "So according to these embeddings, our second training phrase will be represented as:\n",
    "\n",
    "[[0.7, 1.7], [0.1, 4.2], [1.0, 3.1], [0.3, 2.1], [4.1, 2.0]]\n",
    "It might seem counter intuitive at first, but the underlying automatic differentiation engines (e.g., Tensorflow or Theano) manage to optimize these vectors associated to each input integer just like any other parameter of your model\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GRU ( GATED RECURRENT UNIT ): more like LSTM(3) means GRU (2)\n",
    "\n",
    "# 3. Dense :\n",
    "# ----> giving the probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 6. Training the model :\n",
    "    \n",
    "    # one hot encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Generating new text\n",
    "\n",
    "# ----> save our model and load our model weights with different batch size order to pass single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
