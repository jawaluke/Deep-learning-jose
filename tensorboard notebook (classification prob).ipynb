{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframes\n",
    "\n",
    "df = pd.DataFrame(data = load_breast_cancer().data, columns = load_breast_cancer().feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  results  \n",
       "0          0.4601                  0.11890        0  \n",
       "1          0.2750                  0.08902        0  \n",
       "2          0.3613                  0.08758        0  \n",
       "3          0.6638                  0.17300        0  \n",
       "4          0.2364                  0.07678        0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine results columns\n",
    "\n",
    "df[\"results\"] = load_breast_cancer().target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"results\", axis = 1).values\n",
    "y = df[\"results\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.3858755811379235)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.min(), X_test_scaled.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early = EarlyStopping(monitor= \"val_loss\", mode= \"min\", verbose=1, patience = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jawah\\\\Deep learning jose'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd  # location of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. date and time \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d--%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for window : use \"logs\\\\fit\"\n",
    "\n",
    "log_directory = \"logs\\\\fit\"+\"\\\\\"+timestamp\n",
    "\n",
    "board = TensorBoard(\n",
    "                            log_dir = log_directory,\n",
    "                                histogram_freq = 1,\n",
    "                                    write_graph = True,\n",
    "                                        write_images = True,\n",
    "                                            update_freq = \"epoch\",\n",
    "                                                profile_batch = 2,\n",
    "                                                    embeddings_freq = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model layers\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(30, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(15, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss = \"binary_crossentropy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "12/12 [==============================] - 3s 121ms/step - loss: 0.7089 - val_loss: 0.6873\n",
      "Epoch 2/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6787 - val_loss: 0.6648\n",
      "Epoch 3/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6709 - val_loss: 0.6426\n",
      "Epoch 4/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6578 - val_loss: 0.6226\n",
      "Epoch 5/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6534 - val_loss: 0.6062\n",
      "Epoch 6/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6182 - val_loss: 0.5887\n",
      "Epoch 7/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.6071 - val_loss: 0.5718\n",
      "Epoch 8/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5890 - val_loss: 0.5537\n",
      "Epoch 9/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5751 - val_loss: 0.5341\n",
      "Epoch 10/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5677 - val_loss: 0.5155\n",
      "Epoch 11/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5389 - val_loss: 0.4964\n",
      "Epoch 12/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5232 - val_loss: 0.4748\n",
      "Epoch 13/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5214 - val_loss: 0.4497\n",
      "Epoch 14/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5128 - val_loss: 0.4324\n",
      "Epoch 15/300\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.423 - 0s 9ms/step - loss: 0.4794 - val_loss: 0.4136\n",
      "Epoch 16/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4904 - val_loss: 0.3951\n",
      "Epoch 17/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4631 - val_loss: 0.3804\n",
      "Epoch 18/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4717 - val_loss: 0.3642\n",
      "Epoch 19/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4247 - val_loss: 0.3459\n",
      "Epoch 20/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4084 - val_loss: 0.3313\n",
      "Epoch 21/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.4112 - val_loss: 0.3185\n",
      "Epoch 22/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.4205 - val_loss: 0.3068\n",
      "Epoch 23/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3836 - val_loss: 0.2959\n",
      "Epoch 24/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3990 - val_loss: 0.2877\n",
      "Epoch 25/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3508 - val_loss: 0.2769\n",
      "Epoch 26/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3624 - val_loss: 0.2683\n",
      "Epoch 27/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3367 - val_loss: 0.2570\n",
      "Epoch 28/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3630 - val_loss: 0.2497\n",
      "Epoch 29/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3405 - val_loss: 0.2399\n",
      "Epoch 30/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3566 - val_loss: 0.2310\n",
      "Epoch 31/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2935 - val_loss: 0.2221\n",
      "Epoch 32/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3161 - val_loss: 0.2142\n",
      "Epoch 33/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3336 - val_loss: 0.2087\n",
      "Epoch 34/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2851 - val_loss: 0.2033\n",
      "Epoch 35/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2826 - val_loss: 0.2008\n",
      "Epoch 36/300\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.2567 - val_loss: 0.1976\n",
      "Epoch 37/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3054 - val_loss: 0.1943\n",
      "Epoch 38/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2671 - val_loss: 0.1926\n",
      "Epoch 39/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2542 - val_loss: 0.1871\n",
      "Epoch 40/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2639 - val_loss: 0.1776\n",
      "Epoch 41/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2332 - val_loss: 0.1702\n",
      "Epoch 42/300\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.2402 - val_loss: 0.1669\n",
      "Epoch 43/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2693 - val_loss: 0.1638\n",
      "Epoch 44/300\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.2449 - val_loss: 0.1598\n",
      "Epoch 45/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2294 - val_loss: 0.1559\n",
      "Epoch 46/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2240 - val_loss: 0.1541\n",
      "Epoch 47/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2320 - val_loss: 0.1521\n",
      "Epoch 48/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2325 - val_loss: 0.1487\n",
      "Epoch 49/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2246 - val_loss: 0.1442\n",
      "Epoch 50/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1893 - val_loss: 0.1404\n",
      "Epoch 51/300\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.1956 - val_loss: 0.1322\n",
      "Epoch 52/300\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1980 - val_loss: 0.1299\n",
      "Epoch 53/300\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.2290 - val_loss: 0.1282\n",
      "Epoch 54/300\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.2010 - val_loss: 0.1273\n",
      "Epoch 55/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1829 - val_loss: 0.1283\n",
      "Epoch 56/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1910 - val_loss: 0.1240\n",
      "Epoch 57/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1946 - val_loss: 0.1228\n",
      "Epoch 58/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1965 - val_loss: 0.1187\n",
      "Epoch 59/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2042 - val_loss: 0.1156\n",
      "Epoch 60/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1706 - val_loss: 0.1134\n",
      "Epoch 61/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1620 - val_loss: 0.1111\n",
      "Epoch 62/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1867 - val_loss: 0.1093\n",
      "Epoch 63/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1933 - val_loss: 0.1149\n",
      "Epoch 64/300\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.1970 - val_loss: 0.1072\n",
      "Epoch 65/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1808 - val_loss: 0.1042\n",
      "Epoch 66/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1716 - val_loss: 0.1021\n",
      "Epoch 67/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2138 - val_loss: 0.0987\n",
      "Epoch 68/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1422 - val_loss: 0.1027\n",
      "Epoch 69/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1830 - val_loss: 0.0976\n",
      "Epoch 70/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2022 - val_loss: 0.0959\n",
      "Epoch 71/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1569 - val_loss: 0.0949\n",
      "Epoch 72/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1396 - val_loss: 0.0956\n",
      "Epoch 73/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1401 - val_loss: 0.0901\n",
      "Epoch 74/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1154 - val_loss: 0.0893\n",
      "Epoch 75/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1423 - val_loss: 0.0877\n",
      "Epoch 76/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1852 - val_loss: 0.0884\n",
      "Epoch 77/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1958 - val_loss: 0.0882\n",
      "Epoch 78/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1631 - val_loss: 0.0856\n",
      "Epoch 79/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1549 - val_loss: 0.0834\n",
      "Epoch 80/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1324 - val_loss: 0.0847\n",
      "Epoch 81/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1214 - val_loss: 0.0864\n",
      "Epoch 82/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1533 - val_loss: 0.0822\n",
      "Epoch 83/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1557 - val_loss: 0.0817\n",
      "Epoch 84/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1618 - val_loss: 0.0826\n",
      "Epoch 85/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1489 - val_loss: 0.0796\n",
      "Epoch 86/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1241 - val_loss: 0.0789\n",
      "Epoch 87/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1286 - val_loss: 0.0800\n",
      "Epoch 88/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1399 - val_loss: 0.0786\n",
      "Epoch 89/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1555 - val_loss: 0.0771\n",
      "Epoch 90/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1205 - val_loss: 0.0726\n",
      "Epoch 91/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1261 - val_loss: 0.0754\n",
      "Epoch 92/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1420 - val_loss: 0.0729\n",
      "Epoch 93/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1841 - val_loss: 0.0727\n",
      "Epoch 94/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1740 - val_loss: 0.0719\n",
      "Epoch 95/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1430 - val_loss: 0.0749\n",
      "Epoch 96/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1472 - val_loss: 0.0728\n",
      "Epoch 97/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1370 - val_loss: 0.0766\n",
      "Epoch 98/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1397 - val_loss: 0.0754\n",
      "Epoch 99/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1512 - val_loss: 0.0722\n",
      "Epoch 100/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1451 - val_loss: 0.0723\n",
      "Epoch 101/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1161 - val_loss: 0.0747\n",
      "Epoch 102/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1299 - val_loss: 0.0707\n",
      "Epoch 103/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1309 - val_loss: 0.0697\n",
      "Epoch 104/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1224 - val_loss: 0.0701\n",
      "Epoch 105/300\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.1208 - val_loss: 0.0680\n",
      "Epoch 106/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1155 - val_loss: 0.0663\n",
      "Epoch 107/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1695 - val_loss: 0.0656\n",
      "Epoch 108/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1364 - val_loss: 0.0694\n",
      "Epoch 109/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1597 - val_loss: 0.0667\n",
      "Epoch 110/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1324 - val_loss: 0.0655\n",
      "Epoch 111/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1085 - val_loss: 0.0642\n",
      "Epoch 112/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1152 - val_loss: 0.0632\n",
      "Epoch 113/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1367 - val_loss: 0.0645\n",
      "Epoch 114/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1235 - val_loss: 0.0645\n",
      "Epoch 115/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1496 - val_loss: 0.0622\n",
      "Epoch 116/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1030 - val_loss: 0.0632\n",
      "Epoch 117/300\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 0.1556 - val_loss: 0.0639\n",
      "Epoch 118/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1192 - val_loss: 0.0677\n",
      "Epoch 119/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1300 - val_loss: 0.0628\n",
      "Epoch 120/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1172 - val_loss: 0.0615\n",
      "Epoch 121/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0831 - val_loss: 0.0596\n",
      "Epoch 122/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1454 - val_loss: 0.0592\n",
      "Epoch 123/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1107 - val_loss: 0.0630\n",
      "Epoch 124/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1652 - val_loss: 0.0597\n",
      "Epoch 125/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1236 - val_loss: 0.0643\n",
      "Epoch 126/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1100 - val_loss: 0.0698\n",
      "Epoch 127/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1002 - val_loss: 0.0646\n",
      "Epoch 128/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1258 - val_loss: 0.0611\n",
      "Epoch 129/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1184 - val_loss: 0.0642\n",
      "Epoch 130/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0874 - val_loss: 0.0650\n",
      "Epoch 131/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0915 - val_loss: 0.0570\n",
      "Epoch 132/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1113 - val_loss: 0.0570\n",
      "Epoch 133/300\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0979 - val_loss: 0.0552\n",
      "Epoch 134/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1153 - val_loss: 0.0595\n",
      "Epoch 135/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1118 - val_loss: 0.0563\n",
      "Epoch 136/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1124 - val_loss: 0.0595\n",
      "Epoch 137/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1126 - val_loss: 0.0559\n",
      "Epoch 138/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1106 - val_loss: 0.0612\n",
      "Epoch 139/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0991 - val_loss: 0.0606\n",
      "Epoch 140/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1108 - val_loss: 0.0563\n",
      "Epoch 141/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0968 - val_loss: 0.0739\n",
      "Epoch 142/300\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0865 - val_loss: 0.0582\n",
      "Epoch 143/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1362 - val_loss: 0.0536\n",
      "Epoch 144/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0945 - val_loss: 0.0550\n",
      "Epoch 145/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0728 - val_loss: 0.0609\n",
      "Epoch 146/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0807 - val_loss: 0.0548\n",
      "Epoch 147/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1088 - val_loss: 0.0590\n",
      "Epoch 148/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1092 - val_loss: 0.0634\n",
      "Epoch 149/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1079 - val_loss: 0.0509\n",
      "Epoch 150/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0739 - val_loss: 0.0534\n",
      "Epoch 151/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1268 - val_loss: 0.0525\n",
      "Epoch 152/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0831 - val_loss: 0.0533\n",
      "Epoch 153/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1065 - val_loss: 0.0559\n",
      "Epoch 154/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0650 - val_loss: 0.0574\n",
      "Epoch 155/300\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0814 - val_loss: 0.0625\n",
      "Epoch 156/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0843 - val_loss: 0.0624\n",
      "Epoch 157/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1319 - val_loss: 0.0561\n",
      "Epoch 158/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1006 - val_loss: 0.0544\n",
      "Epoch 159/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0883 - val_loss: 0.0494\n",
      "Epoch 160/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1052 - val_loss: 0.0487\n",
      "Epoch 161/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1226 - val_loss: 0.0550\n",
      "Epoch 162/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0686 - val_loss: 0.0624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1073 - val_loss: 0.0524\n",
      "Epoch 164/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1000 - val_loss: 0.0650\n",
      "Epoch 165/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0862 - val_loss: 0.0543\n",
      "Epoch 166/300\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.074 - 0s 10ms/step - loss: 0.0701 - val_loss: 0.0550\n",
      "Epoch 167/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0949 - val_loss: 0.0578\n",
      "Epoch 168/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0967 - val_loss: 0.0530\n",
      "Epoch 169/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0782 - val_loss: 0.0527\n",
      "Epoch 170/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0863 - val_loss: 0.0571\n",
      "Epoch 171/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0917 - val_loss: 0.0573\n",
      "Epoch 172/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1015 - val_loss: 0.0595\n",
      "Epoch 173/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0957 - val_loss: 0.0698\n",
      "Epoch 174/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0681 - val_loss: 0.0493\n",
      "Epoch 175/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1008 - val_loss: 0.0462\n",
      "Epoch 176/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0857 - val_loss: 0.0485\n",
      "Epoch 177/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0899 - val_loss: 0.0553\n",
      "Epoch 178/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1150 - val_loss: 0.0485\n",
      "Epoch 179/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0908 - val_loss: 0.0521\n",
      "Epoch 180/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0872 - val_loss: 0.0545\n",
      "Epoch 181/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0823 - val_loss: 0.0503\n",
      "Epoch 182/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0931 - val_loss: 0.0518\n",
      "Epoch 183/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0900 - val_loss: 0.0545\n",
      "Epoch 184/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1245 - val_loss: 0.0566\n",
      "Epoch 185/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0914 - val_loss: 0.0553\n",
      "Epoch 186/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0840 - val_loss: 0.0556\n",
      "Epoch 187/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0750 - val_loss: 0.0488\n",
      "Epoch 188/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0948 - val_loss: 0.0565\n",
      "Epoch 189/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0783 - val_loss: 0.0516\n",
      "Epoch 190/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0927 - val_loss: 0.0527\n",
      "Epoch 191/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0583 - val_loss: 0.0568\n",
      "Epoch 192/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0676 - val_loss: 0.0541\n",
      "Epoch 193/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0847 - val_loss: 0.0524\n",
      "Epoch 194/300\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0509 - val_loss: 0.0493\n",
      "Epoch 195/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0821 - val_loss: 0.0495\n",
      "Epoch 196/300\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.0685 - val_loss: 0.0528\n",
      "Epoch 197/300\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0732 - val_loss: 0.0491\n",
      "Epoch 198/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1251 - val_loss: 0.0521\n",
      "Epoch 199/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.0636 - val_loss: 0.0539\n",
      "Epoch 200/300\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1061 - val_loss: 0.0606\n",
      "Epoch 00200: early stopping\n"
     ]
    }
   ],
   "source": [
    "Model = model.fit(X_train_scaled, y_train, epochs = 300, validation_data=(X_test_scaled, y_test), callbacks= [early, board], verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. open cmd with path (pwd in notebook) or conda\\n\\n2. go to path\\n\\n3. run \"tensorboard --logdir logs\\x0cit\\x821-01-15--1306 <---(log file directory)\"\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. open cmd with path (pwd in notebook) or conda\n",
    "\n",
    "2. go to path\n",
    "\n",
    "3. run \"tensorboard --logdir logs\\fit\\2021-01-15--1306 <---(log file directory)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jawah\\\\Deep learning jose'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs\\\\fit\\\\2021-01-15--1306'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_directory   # log file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nand then open browser (http://localhost:6006/)\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "and then open browser (http://localhost:6006/)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
